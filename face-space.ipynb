{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646c880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from pet_loader import PetLoader\n",
    "import classifier as convnet\n",
    "import torch\n",
    "from pet_dataset import PetDataset, ToTensorGray, ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import importlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf3dbe1",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9ef4b8",
   "metadata": {},
   "source": [
    "This section instantiates and trains (if necessary) a classifier. The classifier will be used to assign a classification to the images generated by the autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce42be3",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd31ef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (64,64)\n",
    "l = PetDataset(root=\"afhq/train\", transform=ToTensorGray(), shape=shape)\n",
    "loader = DataLoader(l, batch_size=32, shuffle=False, num_workers=0)\n",
    "model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50169fc6",
   "metadata": {},
   "source": [
    "### Select the best available device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efd56c9",
   "metadata": {},
   "source": [
    "If the machine this is run on has cuda support (meaning a reasonably recent nVidia GPU), the `cuda:0` device is selected. Otherwise this falls back to the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ddd2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b8b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752d2f65",
   "metadata": {},
   "source": [
    "### Instantiate the model, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3410ca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None:\n",
    "    autoencoder = importlib.reload(convnet)\n",
    "\n",
    "model = convnet.Classifier(shape)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22236ee5",
   "metadata": {},
   "source": [
    "### Load the model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373577a8",
   "metadata": {},
   "source": [
    "If the trained parameters were saved previously, load them here. If you want to re-train the model, delete the file `classifier.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbbf8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"classifier.pth\"):\n",
    "    model.load_state_dict(torch.load(\"classifier.pth\"))\n",
    "    model.eval()\n",
    "    print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a12156",
   "metadata": {},
   "source": [
    "### Move the model to the device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e1972a",
   "metadata": {},
   "source": [
    "This is only really necessary for a cuda device as it is already available to the CPU, but calling this makes the code agnostic to the hardware it is running on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77d31b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fb05fc",
   "metadata": {},
   "source": [
    "### Train the model (if necessary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed199190",
   "metadata": {},
   "source": [
    "By default, if the saved parameters are available, training is skipped. If you would like to continue training the model, set `continue_training = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06eac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_training = False\n",
    "\n",
    "if not os.path.exists(\"classifier.pth\") or continue_training:  # change this to True to continue training the model\n",
    "    n_epochs = 100\n",
    "    losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0.0\n",
    "        for i in loader:\n",
    "            imgs = i[\"image\"]\n",
    "            labels = i[\"label\"]\n",
    "            imgs = imgs.to(device, dtype=torch.float).reshape((-1, 1, *shape))\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            #print(outputs.shape)\n",
    "            #print(labels.shape)\n",
    "            loss = criterion(outputs, labels.to(device, dtype=torch.float))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*imgs.size(0)\n",
    "        train_loss = train_loss / len(loader)\n",
    "        print(f\"Epoch {epoch}: loss: {train_loss}\")\n",
    "        losses.append(train_loss)\n",
    "        if len(losses) > 2 and len(losses) % 20 == 0:\n",
    "            plt.plot(losses)\n",
    "            plt.show()\n",
    "    torch.save(model.state_dict(), \"classifier.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594bd9a1",
   "metadata": {},
   "source": [
    "### Test the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c30d6a",
   "metadata": {},
   "source": [
    "This is not actually a proper test of the model and if we cared more about an accurate measure of its performance, we would have segregated the data into train, test, and validate datasets. \n",
    "\n",
    "Because we don't care about the performance of this model on the original problem space (pictures of cats / dogs / wild animals), and instead want to apply it to a different problem space (the output of the variational autoencoder), this isn't a major issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c9702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a test batch\n",
    "itr = iter(loader)\n",
    "imgs = next(itr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5348c389",
   "metadata": {},
   "source": [
    "### Get some images / predictions and display them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2425653",
   "metadata": {},
   "source": [
    "Here we just grab some images from the dataset and then use the trained model to label them. This is really just to allow a visual inspection of the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeb7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(imgs[\"image\"].to(device, dtype=torch.float).reshape((-1, 1, *shape)))\n",
    "output = output.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3375815",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_map = {0: \"cat\", 1: \"dog\", 2: \"wild\"}\n",
    "for i in range(output.shape[0]):\n",
    "    print(class_map[int(torch.argmax(torch.tensor(output[i])))])\n",
    "    plt.imshow(imgs[\"image\"][i].detach().numpy(), cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a14c1c8",
   "metadata": {},
   "source": [
    "## Load the Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241c2f10",
   "metadata": {},
   "source": [
    "The variational autoencoder (VAE) is trained on the same data as the classifier, but instead of attempting to assign labels to the data, the model is meant to generate an image similar to the input.\n",
    "\n",
    "Autoencoders are artificial neural networks (ANNs) that are composed of two elements (which themselves are composed of several layers each):\n",
    "\n",
    "- Decoder\n",
    "    - Transforms the input to some other internal representation. Here, we represent any input image by a pair of floating point numbers (i.e. [1.034, 6.221]).\n",
    "- Encoder\n",
    "    - Transforms the internal representations to a variation on the input. Here, we want the model to generate an output image of the same dimensions as the input with characteristics similar to the input image.\n",
    "\n",
    "Two floating point numbers represents a significant compression of the original input space and likely negatively impacts the performance of the VAE in producing variations on the input, but for this demonstration this format was chosen for a very specific reason: after training, the network will be split apart and a function will iterate over values of $x$ and $y$, which will be fed to the Encoder. This will allow us to map the space of the derived internal representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74166930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15310b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vae as autoencoder\n",
    "vae_model = None\n",
    "if vae_model is not None:\n",
    "    autoencoder = importlib.reload(autoencoder)\n",
    "vae_model = autoencoder.VAE(shape=shape)\n",
    "if os.path.isfile(\"vae.pth\"):\n",
    "    vae_model.load_state_dict(torch.load(\"vae.pth\"))\n",
    "    model.eval()\n",
    "vae_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb85166",
   "metadata": {},
   "source": [
    "### Define a suitable loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915d4da0",
   "metadata": {},
   "source": [
    "This is a loss function that has been proposed for variational autoencoders. I haven't verified this is optimal for the kind of output this VAE produces and we might get better results from a different loss function, but this worked well enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04366ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, prod(shape)), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2140065",
   "metadata": {},
   "source": [
    "### Select the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a214b1ec",
   "metadata": {},
   "source": [
    "The ADAM optimization algorithm is one of the most commonly used optimization functions. In essence, it reduces the learning rate each epoch allowing the optimization to proceed down gradients without overshooting the minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d8e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "vae_optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd51b53",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631e8431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(epoch, vae_loader):\n",
    "    vae_model.train()\n",
    "    vae_train_loss = 0\n",
    "    for i in vae_loader:\n",
    "        data = i[\"image\"].to(device, dtype=torch.float)\n",
    "        vae_optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = vae_model(data)\n",
    "        vae_loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        vae_loss.backward()\n",
    "        vae_train_loss += vae_loss.item()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "              epoch, vae_train_loss / len(vae_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818c63f8",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd70ff",
   "metadata": {},
   "source": [
    "If the model is not trained or `continue_training_vae is True` train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735c4148",
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_training_vae = False\n",
    "if not os.path.isfile(\"vae.pth\") or continue_training_vae:\n",
    "    vae_l = PetDataset(root=\"afhq\", transform=ToTensorGray(), shape=shape)\n",
    "    vae_loader = DataLoader(vae_l, batch_size=32, shuffle=False, num_workers=0)\n",
    "    for epoch in range(1, 500 + 1):\n",
    "        train(epoch, vae_loader)\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(64, 2).to(device)\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, *shape),\n",
    "                        'results/sample_' + str(epoch) + '.png')\n",
    "    torch.save(model.state_dict(), \"vae.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e09798f",
   "metadata": {},
   "source": [
    "### Test the output of the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2536a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=256\n",
    "input_space = np.append(np.linspace([-1]*d,[1]*d,d), np.linspace([-1]*d,[1]*d,d).T).reshape((2, d, d)).transpose(2, 1, 0)\n",
    "inspace = torch.from_numpy(input_space).to(device, dtype=torch.float)\n",
    "output_space = vae_model.decode(inspace).cpu()\n",
    "output_space_map = np.zeros((d, d, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8235113",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output_space.reshape(-1, 4096)[88].detach().numpy().reshape(64,64), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2325bd10",
   "metadata": {},
   "source": [
    "### Save the output space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49567a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "save_image(output_space.reshape(-1, 1, 64, 64), 'results/variational_space.png', nrow=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faa1498",
   "metadata": {},
   "source": [
    "### Classify the output space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addc0033",
   "metadata": {},
   "source": [
    "Here, we take the individual outputs of the VAE and classify them with the classifier. Because the classifier was trained with real images we can be sure that none of these inputs have been seen by the classifier. The idea, here, is to show that the 2D space of inputs to the Encoder (defined by the value of the $x$ and $y$ values) defines a continuous function where the transition from cat to dog (to wild) is smooth and the regions where each predominates are more or less contiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0092069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(d):\n",
    "    output = model(output_space[i].to(device, dtype=torch.float).reshape((-1, 1, *shape)))\n",
    "    output = output.cpu().detach().numpy()\n",
    "    for j in range(d):\n",
    "        probability = torch.tensor(output[j])[int(torch.argmax(torch.tensor(output[j])))]\n",
    "        class_name = class_map[int(torch.argmax(torch.tensor(output[j])))]\n",
    "        if probability > 0.25:\n",
    "            output_space_map[i, j] = np.array(output[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3b637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d602c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_space_image = PIL.Image.fromarray((output_space_map*255).astype(np.uint8))\n",
    "plt.imshow(output_space_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dfa63d",
   "metadata": {},
   "source": [
    "### Apply the classification to the original grayscale face-space image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b475b65",
   "metadata": {},
   "source": [
    "In order to illustrate the classification of the images generated by the VAE, we scale the classification image such that each pixel is enlarged to cover the image the pixel represents the classification of in order to visually label the images for clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238e0076",
   "metadata": {},
   "source": [
    "#### Convert the stacked output to a 2D space similar to the image saved previously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f534be3",
   "metadata": {},
   "source": [
    "Ideally, we might want to simply load the image of the VAE output space that was saved earlier to ensure that this image is exactly the same, but pillow (`PIL`) refuses to load an image that large so, instead, we reconstruct it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab599c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_map_image(stacked_image, nrow):\n",
    "    sector_width = stacked_image.shape[2]\n",
    "    sector_height = stacked_image.shape[3]\n",
    "    ncol = stacked_image.shape[0] // nrow\n",
    "    output = np.zeros((nrow*sector_width, ncol*sector_height))\n",
    "    for i in range(stacked_image.shape[0]):\n",
    "        x = int((i % nrow) * sector_width)\n",
    "        y = int((i // nrow) * sector_height)\n",
    "        output[x:x+sector_width, y:y+sector_height] = stacked_image[i]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc7ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_faces_image = PIL.Image.fromarray((create_map_image(output_space.detach().numpy().reshape(-1, 1, 64, 64), 256)*255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b9dd25",
   "metadata": {},
   "source": [
    "#### View a small section of the output space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafc930f",
   "metadata": {},
   "source": [
    "Crop and display a small section of the output space because the full image is too large to make sense of all at once. This is merely for visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e7abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output_faces_image.crop((0, 0, 256, 256)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01282ea0",
   "metadata": {},
   "source": [
    "#### Blend the two images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a51fa3",
   "metadata": {},
   "source": [
    "The classification space is derived from the face space image where each image results in a classification that is a 3-value tuple (cat, dog, wild). These three values are represented by red, green, and blue color components.\n",
    "\n",
    "In order to demonstrate the classifications of the images visually, we first scale the classification size to equal the size of the face space. This ensures each pixel is applied to the corresponding face image. This rescaled image is then blended with the face-space image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f2a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "colored_output_faces = PIL.Image.blend(output_space_image.resize(output_faces_image.size).convert(\"RGB\"), output_faces_image.convert(\"RGB\"), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56faa1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(colored_output_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85d0494",
   "metadata": {},
   "source": [
    "#### Save the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097c5c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "colored_output_faces.save(\"results/colored_output_faces.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
